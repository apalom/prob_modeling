{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When reading through this notebook, please also read the python comments carefully. \n",
    "# First, we import tensorflow and other commomly used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the version of numpy and tensorflow\n",
    "print( np.__version__)\n",
    "print( tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check device\n",
    "device_list = device_lib.list_local_devices()\n",
    "for d in device_list:\n",
    "    print( d.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1 Arithmetic and basic tensor operations\n",
    "\n",
    "## we compute the result of the computation graph as follow:\n",
    "## a  = 2, b = 3 constant\n",
    "## c = a + b \n",
    "## d = a * b\n",
    "## e = c - d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a  = 2, b = 3 constant\n",
    "#c = a + b \n",
    "#d = a * b\n",
    "#e = c - d\n",
    "\n",
    "#in numpy\n",
    "a = np.array( 2.0 )\n",
    "b = np.array( 3.0 )\n",
    "\n",
    "c = a + b\n",
    "d = a * b\n",
    "e = c - d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#result showing as expected\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in tensorflow\n",
    "a = tf.constant( 2.0)\n",
    "b = tf.constant( 3.0)\n",
    "\n",
    "#define operations, i.e., compute graph\n",
    "c = a + b\n",
    "d = a * b\n",
    "e = c - d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Until now, we are just defining the graph, not actually excute it.\n",
    "print( c )\n",
    "print( d )\n",
    "print( e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to excute a graph, we need a tensorflow \"engine\" call session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excute the graph, sess only return the value of nodes that been passed to it\n",
    "\n",
    "e_val = sess.run( e)\n",
    "print( e_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_val, d_val, e_val = sess.run([ c, d, e])\n",
    "print( c_val, d_val, e_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule 1: First define the graph then excute it. Graph won't change in excution. \n",
    "## Tensorflow only excute computations(nodes) that are neccessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix computation\n",
    "A = np.array([[1,2],[3,4]])\n",
    "B = np.array([[3,4],[5,6]])\n",
    "C = A @ B\n",
    "\n",
    "print(A,'\\n')\n",
    "print(B,'\\n')\n",
    "print(C,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_A = tf.constant( A)\n",
    "tf_B = tf.constant( B)\n",
    "\n",
    "tf_C = tf_A @ tf_B  \n",
    "print(tf_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_C_val = sess.run( tf_C)\n",
    "print( tf_C_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Place holders: used to represent data points\n",
    "### feed_dicts: replace(feed) placeholders with actual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_A = tf.placeholder( tf.float32, shape=[2,2])\n",
    "tf_B = tf.placeholder( tf.float32, shape = [2,2])\n",
    "\n",
    "tf_C = tf_A @ tf_B\n",
    "print( tf_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_C_val = sess.run( tf_C, feed_dict = { tf_A:A, tf_B:B})\n",
    "\n",
    "#tf_C_val = sess.run( tf_C) # wrong calling\n",
    "print( tf_C_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: tensors whose values can change, i.e., be assigned new values. Usually represent parameters like weights, bias, means and variances.\n",
    "### Node that you can also feed value to a variable to overwrite its current value temporally. Useful in debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_w = tf.Variable( 0.0)\n",
    "tf_b = tf.Variable( 0.0)\n",
    "\n",
    "add_op = tf.assign_add( tf_w, 1.0)\n",
    "print( tf_w)\n",
    "print( tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run( tf.global_variables_initializer()) # initialized variables before using them\n",
    "tf_w_val = sess.run( tf_w)\n",
    "print( tf_w_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sess.run( add_op)\n",
    "print( sess.run( tf_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overwrite the value of w to -2\n",
    "print( sess.run(tf_w, feed_dict={ tf_w:-2}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( sess.run( tf_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule 2: Initalized Variable before excute graph. Note that you only need to initialize variables once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "### Place holders: data points\n",
    "### Variables: parameters\n",
    "### Constants: other fixed values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Bayesian Logistic Regression( MAP )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using banknote data set as we use in homeowork 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess data set\n",
    "train_dat = np.genfromtxt( \"train.csv\", delimiter=\",\")\n",
    "test_dat = np.genfromtxt( \"test.csv\", delimiter=',')\n",
    "\n",
    "x_train = train_dat[:, :-1]\n",
    "y_train = train_dat[:, -1]\n",
    "\n",
    "x_test = test_dat[:, :-1]\n",
    "y_test = test_dat[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.dtype, y_train.dtype, x_test.dtype, y_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribute normalization\n",
    "x_train = x_train.astype( np.float32)\n",
    "x_test = x_test.astype( np.float32)\n",
    "y_train = y_train.astype( np.int32)\n",
    "y_test = y_test.astype( np.int32)\n",
    "\n",
    "mean_train = np.average( x_train, axis=0)\n",
    "var_train = np.var( x_train, axis = 0)\n",
    "\n",
    "x_train -= mean_train\n",
    "x_train /= np.sqrt( var_train )\n",
    "\n",
    "x_test -= mean_train\n",
    "x_test /= np.sqrt( var_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf.fit( x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_clf.predict( x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score( y_test, y_pred)\n",
    "print( 'acc = %g' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_Logistic_Classifier_NR:\n",
    "    def __init__(self, D, zero_init = False, reg = 1.0):\n",
    "        '''\n",
    "        :param D: length of feature vector\n",
    "        :param zero_init: whether use all zero initialization\n",
    "        :param reg: regularization strength ( precision of prior))\n",
    "        '''\n",
    "        self.D = D\n",
    "        self.zero_init = zero_init\n",
    "        self.reg = reg\n",
    "\n",
    "        if self.zero_init:\n",
    "            self.initializer = tf.initializers.zeros( )\n",
    "        else:\n",
    "            self.initializer = tf.initializers.glorot_normal()\n",
    "\n",
    "        self._build_graph()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        #model parameters( weights)\n",
    "        self.w = tf.Variable( self.initializer( shape = [ self.D, 1] ))\n",
    "\n",
    "        #Design Matrix and target label\n",
    "        self.X = tf.placeholder( tf.float32, shape = [None, self.D,])\n",
    "        self.t = tf.placeholder( tf.float32, shape = [None, 1])\n",
    "        self.y = tf.sigmoid( self.X @ self.w)\n",
    "\n",
    "        self.grad = tf.transpose( self.X) @ ( self.y - self.t) + self.w * self.reg\n",
    "\n",
    "        R = tf.linalg.diag( tf.reshape(  self.y * (  1 - self.y),shape=[-1] ))\n",
    "        H = tf.transpose( self.X) @ R @ self.X + tf.eye(self.D)*self.reg\n",
    "        \n",
    "        #key step\n",
    "        self.update_op = tf.assign( self.w, self.w - tf.linalg.inv( H) @ self.grad)\n",
    "\n",
    "        #GPU settings\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.run_options = tf.RunOptions(report_tensor_allocations_upon_oom=True)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def fit(self,X, t, num_iter = 10, verbose = False):\n",
    "        '''\n",
    "        :param X: Design Matrix, N by D\n",
    "        :param t: target, N by None\n",
    "        :return: self\n",
    "        '''\n",
    "\n",
    "        self.train_hist = []\n",
    "        for i in range( num_iter):\n",
    "            train_feed_dict = { self.X:X, self.t:t.reshape(-1,1)}\n",
    "            _,  w = self.sess.run( [ self.update_op, self.w], feed_dict=train_feed_dict)\n",
    "\n",
    "            if verbose:\n",
    "                print('iter %3d: w = ' % ( i +1),w.reshape( -1))\n",
    "        return self\n",
    "\n",
    "    def predict(self,X):\n",
    "        test_feed_dict = { self.X:X}\n",
    "\n",
    "        y_pred = self.sess.run( self.y, feed_dict=test_feed_dict).reshape( -1)\n",
    "        y_pred = ( y_pred > 0.5).astype( np.int)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class TF_Logistic_Classifier_SGD:\n",
    "    def __init__(self, D, zero_init = False, reg = 1.0, lr = 0.1):\n",
    "        '''\n",
    "        :param D: length of feature vector\n",
    "        :param zero_init: whether use all zero initialization\n",
    "        :param reg: regularization strength ( precision of prior))\n",
    "        '''\n",
    "        self.D = D\n",
    "        self.zero_init = zero_init\n",
    "        self.reg = reg\n",
    "        self.lr = lr\n",
    "\n",
    "        if self.zero_init:\n",
    "            self.initializer = tf.initializers.zeros( )\n",
    "        else:\n",
    "            self.initializer = tf.initializers.glorot_normal()\n",
    "\n",
    "        self._build_graph()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        #model parameters( weights)\n",
    "        self.w = tf.Variable( self.initializer( shape = [ self.D,1] ))\n",
    "\n",
    "        #Design Matrix and target label\n",
    "        self.X = tf.placeholder( tf.float32, shape = [None, self.D,])\n",
    "        self.t = tf.placeholder( tf.float32, shape = [None])\n",
    "\n",
    "        self.y = tf.reshape( tf.sigmoid( self.X @ self.w), shape = [-1] )\n",
    "\n",
    "        self.mle_loss = - tf.reduce_sum( self.t * tf.log( self.y) + ( 1 - self.t) * tf.log( 1 - self.y) )\n",
    "        self.prior_loss = tf.reduce_sum( self.w * self.w * self.reg)\n",
    "        self.map_loss = self.mle_loss + self.prior_loss\n",
    "\n",
    "        #minimizer\n",
    "        self.min_opt = tf.train.AdamOptimizer(self.lr)\n",
    "\n",
    "        #minimizing step\n",
    "        self.min_step = self.min_opt.minimize( self.map_loss)\n",
    "\n",
    "        #GPU settings\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.run_options = tf.RunOptions(report_tensor_allocations_upon_oom=True)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def fit(self,X, t, num_iter = 10, verbose = True):\n",
    "        '''\n",
    "        :param X: Design Matrix, N by D\n",
    "        :param t: target, N by None\n",
    "        :return: self\n",
    "        '''\n",
    "\n",
    "        self.train_hist = []\n",
    "        for i in range( num_iter):\n",
    "            train_feed_dict = { self.X:X, self.t:t}\n",
    "            _, map_loss,w = self.sess.run( [ self.min_step, self.map_loss, self.w], feed_dict=train_feed_dict)\n",
    "            if verbose:\n",
    "                print(\"iter %3d: map loss = %f\" % ( i + 1, map_loss), \" w = \", w.reshape( -1))\n",
    "\n",
    "            self.train_hist.append( map_loss)\n",
    "        return self\n",
    "\n",
    "    def predict(self,X):\n",
    "        test_feed_dict = { self.X:X}\n",
    "\n",
    "        y_pred = self.sess.run( self.y, feed_dict=test_feed_dict)\n",
    "        y_pred = ( y_pred > 0.5).astype( np.int)\n",
    "\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_clf = TF_Logistic_Classifier_NR( x_train.shape[1], zero_init = False, reg = 1.0)\n",
    "_ = NR_clf.fit( x_train, y_train,verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = accuracy_score( y_train, NR_clf.predict( x_train))\n",
    "test_acc = accuracy_score( y_test, NR_clf.predict( x_test))\n",
    "print( \"train acc = %g, test acc =%g\" % ( train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_clf = TF_Logistic_Classifier_SGD( x_train.shape[1], zero_init=False, reg= 1.0, lr= 0.1)\n",
    "_ = SGD_clf.fit( x_train, y_train, num_iter = 50, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_train_acc = accuracy_score( y_train, SGD_clf.predict( x_train))\n",
    "SGD_test_acc = accuracy_score( y_test, SGD_clf.predict( x_test))\n",
    "print( 'SGD trian acc = %g, test acc =%g' % ( SGD_train_acc, SGD_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 8,6\n",
    "plt.plot( range( len( SGD_clf.train_hist)), SGD_clf.train_hist)\n",
    "plt.xlabel( 'num iter')\n",
    "plt.ylabel(' map loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
